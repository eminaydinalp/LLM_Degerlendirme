# LLaMA 1B Fine-Tuning SÃ¼reci - DetaylÄ± Notlar

**Tarih:** 18 Ekim 2025  
**Model:** meta-llama/Llama-3.2-1B-Instruct  
**AmaÃ§:** A1 seviyesi Ä°ngilizce kelimeler iÃ§in cÃ¼mle Ã¼retimi

---

## ğŸ“‹ Ä°Ã§indekiler

1. [Genel BakÄ±ÅŸ](#genel-bakÄ±ÅŸ)
2. [Sorun ve Ã‡Ã¶zÃ¼m SÃ¼reci](#sorun-ve-Ã§Ã¶zÃ¼m-sÃ¼reci)
3. [EÄŸitim Verisi HazÄ±rlama](#eÄŸitim-verisi-hazÄ±rlama)
4. [Fine-Tuning Parametreleri](#fine-tuning-parametreleri)
5. [SonuÃ§lar](#sonuÃ§lar)
6. [KullanÄ±m](#kullanÄ±m)

---

## ğŸ¯ Genel BakÄ±ÅŸ

### Proje Hedefi
A1 seviyesinde 10 Ä°ngilizce kelime verildiÄŸinde, her kelime iÃ§in uygun seviyede Ã¶rnek cÃ¼mle Ã¼reten bir model geliÅŸtirmek.

### KullanÄ±lan Teknoloji
- **Framework:** text-generation-webui + Training PRO extension
- **Method:** LoRA (Low-Rank Adaptation)
- **Base Model:** LLaMA 3.2 1B Instruct (Transformers formatÄ±)

---

## ğŸ”§ Sorun ve Ã‡Ã¶zÃ¼m SÃ¼reci

### Ä°lk Denemeler ve KarÅŸÄ±laÅŸÄ±lan Sorunlar

#### 1. **GGUF Model Problemi**
**Sorun:**
- Ä°lk baÅŸta `Llama-3-2-3B-Instruct-Q4_K_M.gguf` (GGUF formatÄ±) kullanÄ±ldÄ±
- llama.cpp model loader ile yÃ¼klendi
- Training baÅŸlatÄ±lÄ±nca hata: `AttributeError: 'LlamaServer' object has no attribute 'bos_token_id'`

**Neden:**
- GGUF formatÄ± sadece inference iÃ§in (quantized, sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ)
- Fine-tuning yapÄ±lamaz
- llama.cpp training desteklemiyor

**Ã‡Ã¶zÃ¼m:**
- Transformers formatÄ±nda model gerekli
- LLaMA 3.2 1B Instruct Transformers versiyonu indirildi

#### 2. **Model Ä°ndirme Sorunu**
**Sorun:**
- Hugging Face'den indirme sÄ±rasÄ±nda 401/403 hatalarÄ±

**Ã‡Ã¶zÃ¼m AdÄ±mlarÄ±:**
```bash
# 1. Hugging Face token oluÅŸtur
# https://huggingface.co/settings/tokens
# Token Type: Read

# 2. CLI ile giriÅŸ yap
huggingface-cli login
# Token gir

# 3. LLaMA modeline eriÅŸim izni al
# https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
# "Agree and access repository" butonuna tÄ±kla

# 4. Model indir
cd /home/user/text-generation-webui
source venv/bin/activate
python download-model.py meta-llama/Llama-3.2-1B-Instruct
```

#### 3. **Training PRO API UyumsuzluÄŸu**
**Sorun:**
```
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
```

**Neden:**
- Transformers kÃ¼tÃ¼phanesi gÃ¼ncellendi
- `evaluation_strategy` â†’ `eval_strategy` olarak deÄŸiÅŸti

**Ã‡Ã¶zÃ¼m:**
```bash
sed -i 's/evaluation_strategy=/eval_strategy=/g' \
  /home/user/text-generation-webui/extensions/Training_PRO/script.py
```

#### 4. **Prompt Format UyumsuzluÄŸu**
**Sorun:**
- Ä°lk eÄŸitim verisi: Tek kelime â†’ tek cÃ¼mle formatÄ±
- Test prompt'u: 10 kelime â†’ 10 numaralÄ± cÃ¼mle formatÄ±
- Model beklentiye uygun cevap veremedi

**Ã‡Ã¶zÃ¼m:**
- EÄŸitim verisini yeniden formatla
- Test prompt'uyla uyumlu hale getir

---

## ğŸ“Š EÄŸitim Verisi HazÄ±rlama

### Veri KaynaÄŸÄ±
- **Dosya:** `training_data_a1.json`
- **Ä°Ã§erik:** 4490 entry, 897 farklÄ± kelime
- **Format:** Her entry bir kelime ve Ã¶rnek cÃ¼mle iÃ§eriyor

### Veri DÃ¶nÃ¼ÅŸÃ¼m Scripti

**Dosya:** `create_list_format_training_data.py`

**Strateji:**
- **70% (1400 Ã¶rnek):** 10 kelime - Ana kullanÄ±m senaryosu
- **20% (400 Ã¶rnek):** 8-9 kelime - YakÄ±n varyasyonlar
- **10% (200 Ã¶rnek):** 6-7 kelime - KÃ¼Ã§Ã¼k varyasyonlar

**Toplam:** 2000 eÄŸitim Ã¶rneÄŸi

### Prompt FormatÄ±

```
### Instruction:
Generate A1-level English sentences for these 10 words:
1. word1
2. word2
3. word3
...
10. word10

Provide numbered sentences (1-10), using each word naturally and appropriately for A1 level.

### Response:
1. Sentence for word1.
2. Sentence for word2.
3. Sentence for word3.
...
10. Sentence for word10.
```

### Veri OluÅŸturma Komutu

```bash
cd /home/user/Documents/Tez/Deneyler/LLM_Degerlendirme/notebooks/fine_tuning
python3 create_list_format_training_data.py

# Ã‡Ä±ktÄ± dosyalarÄ±:
# - training_data_a1_list_format.json
# - training_data_a1_list_format.txt

# text-generation-webui'ye kopyala
cp training_data_a1_list_format.txt \
  /home/user/text-generation-webui/user_data/training/datasets/
```

---

## âš™ï¸ Fine-Tuning Parametreleri

### Model AyarlarÄ±
- **Model:** meta-llama/Llama-3.2-1B-Instruct
- **Model Loader:** Transformers
- **Model Class:** LlamaForCausalLM

### LoRA Parametreleri

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Name** | llama1b-a1 | LoRA model adÄ± |
| **LoRA Rank** | 128 | Rank boyutu (yÃ¼ksek = daha fazla parametre) |
| **LoRA Alpha** | 256 | Scaling factor (genelde rank Ã— 2) |
| **LoRA Target Projections** | q+v | Query ve Value projection'larÄ± |

### Training Parametreleri

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Epochs** | 3 | Veri seti Ã¼zerinden 3 kez geÃ§iÅŸ |
| **Learning Rate** | 2e-4 | Ã–ÄŸrenme hÄ±zÄ± |
| **LR Scheduler** | cosine | Cosine annealing |
| **True Batch Size** | 16 | GerÃ§ek batch boyutu |
| **Gradient Accumulation** | 4 | 4 adÄ±mda biriktir |
| **Warmup Steps** | 100 | Ä°lk 100 adÄ±m Ä±sÄ±nma |

### Dataset AyarlarÄ±

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Text File** | training_data_a1_list_format.txt | EÄŸitim dosyasÄ± |
| **Add Overlapping Blocks** | âœ… | Veri artÄ±rma |
| **Chunk Length** | 256 | Token uzunluÄŸu |
| **Hard Cut String** | \n\n\n | Kesme ayÄ±rÄ±cÄ±sÄ± |

### Checkpoint AyarlarÄ±

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Save every N steps** | 0 (OFF) | Manuel kaydetme |
| **Save at 10% Loss Change** | 1.8 | Loss %10 dÃ¼ÅŸÃ¼nce kaydet |

### Advanced Options

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Warmup Steps** | 100 | Learning rate warm-up |
| **Optimizer** | adamw_torch | Adam optimizer |
| **Add BOS token** | âœ… | Beginning-of-sequence token ekle |
| **Add EOS token** | âœ… | End-of-sequence token ekle |
| **LoRA Dropout** | 0.05 | Dropout oranÄ± |

---

## ğŸ“ˆ SonuÃ§lar

### EÄŸitim Ä°statistikleri

```json
{
  "base_model_name": "meta-llama_Llama-3.2-1B-Instruct",
  "base_model_class": "LlamaForCausalLM",
  "loss": 0.9954,
  "train_loss": 1.2828,
  "epoch": 3.0,
  "total_steps": 848,
  "train_runtime": 205.82 saniye (~3.5 dakika),
  "train_samples_per_second": 65.445,
  "train_steps_per_second": 1.035
}
```

### Checkpoints

EÄŸitim sÄ±rasÄ±nda kaydedilen ara modeller (loss deÄŸerleriyle):

```
checkpoint-72-loss-1_62
checkpoint-112-loss-1_51
checkpoint-176-loss-1_40
checkpoint-220-loss-1_27
checkpoint-236-loss-1_16
checkpoint-292-loss-1_06
checkpoint-648-loss-0_97   â† En iyi checkpoint
```

**Loss GrafiÄŸi:** `/home/user/text-generation-webui/user_data/loras/llama1b-a1/training_graph.png`

### Dosya KonumlarÄ±

```
/home/user/text-generation-webui/user_data/loras/llama1b-a1/
â”œâ”€â”€ adapter_config.json          # LoRA konfigÃ¼rasyonu
â”œâ”€â”€ adapter_model.bin            # LoRA aÄŸÄ±rlÄ±klarÄ± (53 MB)
â”œâ”€â”€ training_log.json            # EÄŸitim Ã¶zet logu
â”œâ”€â”€ training_graph.png           # Loss grafiÄŸi (gÃ¶rsel)
â”œâ”€â”€ training_graph.json          # Grafik verisi
â”œâ”€â”€ training_parameters.json     # KullanÄ±lan parametreler
â”œâ”€â”€ README.md                    # Otomatik oluÅŸturulan README
â””â”€â”€ checkpoint-*/                # Ara kayÄ±tlar
```

---

## ğŸš€ KullanÄ±m

### 1. Model YÃ¼kleme

#### Web ArayÃ¼zÃ¼nde:

1. **Model** sekmesine git
2. **Unload** butonu ile mevcut modeli kaldÄ±r (eÄŸer yÃ¼klÃ¼yse)
3. Model dropdown'dan **meta-llama_Llama-3.2-1B-Instruct** seÃ§
4. **Load** butonuna tÄ±kla
5. **LoRA(s)** bÃ¶lÃ¼mÃ¼ne in
6. Dropdown'dan **llama1b-a1** seÃ§
7. **Apply LoRAs** butonuna tÄ±kla
8. "Successfully loaded" mesajÄ±nÄ± bekle

### 2. Test Etme

#### Chat Sekmesinde:

**DoÄŸru Prompt FormatÄ±:**
```
Generate A1-level English sentences for these 10 words:
1. age
2. animal
3. ask
4. computer
5. eat
6. car
7. but
8. drive
9. amazing
10. funny

Provide numbered sentences (1-10), using each word naturally and appropriately for A1 level.
```

**Beklenen Ã‡Ä±ktÄ±:**
```
1. My grandmother is 80 years old.
2. I like animals.
3. Can I ask a question?
4. I use my computer for homework.
5. I eat breakfast every morning.
6. My father has a red car.
7. I like pizza, but I don't like vegetables.
8. I can drive a car.
9. The sunset is amazing.
10. That movie was very funny.
```

### 3. FarklÄ± Kelime SayÄ±larÄ± Ä°Ã§in

Model 6-10 kelime arasÄ± esnek Ã§alÄ±ÅŸÄ±r:

**6 kelime Ã¶rneÄŸi:**
```
Generate A1-level English sentences for these 6 words:
1. happy
2. book
3. friend
4. water
5. school
6. play

Provide numbered sentences (1-6), using each word naturally and appropriately for A1 level.
```

---

## ğŸ“ Ã–nemli Notlar

### âœ… BaÅŸarÄ±lÄ± Olan Åeyler

1. **Hibrit veri stratejisi** - %70 10-kelime, %30 varyasyon
2. **NumaralÄ± liste formatÄ±** - Model yapÄ±yÄ± Ã¶ÄŸrendi
3. **LoRA rank 128** - Yeterince kapasiteli ama hafif
4. **3 epoch** - Overfitting olmadan Ã¶ÄŸrendi

### âš ï¸ Dikkat Edilmesi Gerekenler

1. **Model formatÄ± Ã¶nemli**: GGUF deÄŸil, Transformers formatÄ± gerekli
2. **Prompt tutarlÄ±lÄ±ÄŸÄ±**: EÄŸitim ve test prompt'larÄ± aynÄ± formatta olmalÄ±
3. **LoRA yÃ¼kleme**: EÄŸitim sonrasÄ± mutlaka model reload et
4. **API gÃ¼ncellemeleri**: Training PRO script'i bazen gÃ¼ncelleme gerektirebilir

### ğŸ”„ Gelecekte Tekrar EÄŸitim Ä°Ã§in

**HÄ±zlÄ± BaÅŸlangÄ±Ã§:**

```bash
# 1. Veriyi hazÄ±rla
cd /home/user/Documents/Tez/Deneyler/LLM_Degerlendirme/notebooks/fine_tuning
python3 create_list_format_training_data.py

# 2. Kopyala
cp training_data_a1_list_format.txt \
  /home/user/text-generation-webui/user_data/training/datasets/

# 3. Web arayÃ¼zÃ¼nde:
# - Model yÃ¼kle (base model)
# - Training PRO sekmesi
# - Dataset seÃ§
# - Parametreleri ayarla (yukarÄ±daki tabloya bak)
# - Start LoRA Training
```

---

## ğŸ› Sorun Giderme

### Hata: "evaluation_strategy" deprecated
```bash
sed -i 's/evaluation_strategy=/eval_strategy=/g' \
  /home/user/text-generation-webui/extensions/Training_PRO/script.py
```

### Hata: Dataset gÃ¶rÃ¼nmÃ¼yor
```bash
# DoÄŸru klasÃ¶re kopyala
cp *.txt /home/user/text-generation-webui/user_data/training/datasets/

# Web arayÃ¼zÃ¼nÃ¼ yenile (F5)
```

### Hata: Model dirty after training
```
# Model sekmesinde:
1. Unload butonu
2. Load butonu
3. LoRA'yÄ± tekrar yÃ¼kle
```

---

## ğŸ“š Referanslar

- **Base Model:** https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
- **text-generation-webui:** https://github.com/oobabooga/text-generation-webui
- **LoRA Paper:** https://arxiv.org/abs/2106.09685

---

## ğŸ“Š Veri Setleri

### Kaynak Veri
- `training_data_a1.json` - 4490 entry, 897 kelime

### OluÅŸturulan Veriler
- `training_data_a1_list_format.json` - 2000 Ã¶rnek (JSON)
- `training_data_a1_list_format.txt` - 2000 Ã¶rnek (Text format)

### Scripter
- `create_list_format_training_data.py` - Veri oluÅŸturma scripti
- `convert_to_txt.py` - JSON â†’ TXT dÃ¶nÃ¼ÅŸtÃ¼rme
- `convert_to_webui_format.py` - WebUI format dÃ¶nÃ¼ÅŸtÃ¼rme

---

**Son GÃ¼ncelleme:** 18 Ekim 2025  
**Durum:** âœ… BaÅŸarÄ±lÄ± - Model eÄŸitildi ve test edildi  
**Sonraki AdÄ±mlar:** FarklÄ± seviyelerde (A2, B1, B2, C1) aynÄ± sÃ¼reci tekrarla
