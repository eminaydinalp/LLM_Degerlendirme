"""
Bilimsel GeÃ§erlilik ve GÃ¼venilirlik Analizi
Bu script, anket sonuÃ§larÄ±nÄ±n bilimsel geÃ§erliliÄŸini deÄŸerlendirir.
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import kruskal, friedmanchisquare, shapiro, levene
import warnings
warnings.filterwarnings('ignore')

# Dosya yollarÄ±
DATA_FILE = "all_ratings.csv"
OUTPUT_FILE = "scientific_validity_report.md"

def load_data():
    """Verileri yÃ¼kle"""
    df = pd.read_csv(DATA_FILE)
    return df

def calculate_inter_rater_reliability(df):
    """DeÄŸerlendiriciler arasÄ± gÃ¼venilirlik - Krippendorff's Alpha benzeri"""
    
    # Her cÃ¼mle-kriter kombinasyonu iÃ§in
    results = []
    
    for (word, model, sentence, criterion), group in df.groupby(['word', 'model', 'sentence', 'criterion']):
        ratings = group['rating'].values
        n_raters = len(ratings)
        
        if n_raters > 1:
            # Varyans hesapla
            variance = np.var(ratings, ddof=1)
            mean_rating = np.mean(ratings)
            std_rating = np.std(ratings, ddof=1)
            
            # Coefficient of Variation
            cv = (std_rating / mean_rating * 100) if mean_rating > 0 else 0
            
            results.append({
                'word': word,
                'model': model,
                'criterion': criterion,
                'n_raters': n_raters,
                'mean': mean_rating,
                'std': std_rating,
                'variance': variance,
                'cv': cv
            })
    
    reliability_df = pd.DataFrame(results)
    return reliability_df

def calculate_cronbach_alpha(df):
    """Cronbach's Alpha - Ä°Ã§ tutarlÄ±lÄ±k"""
    
    # Her model iÃ§in 4 kriterin korelasyonu
    results = []
    
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        
        # Pivot table: her satÄ±r bir item (cÃ¼mle), her sÃ¼tun bir kriter
        pivot = model_data.pivot_table(
            values='rating',
            index=['word', 'sentence_label', 'participant_id'],
            columns='criterion',
            aggfunc='first'
        )
        
        if len(pivot.columns) == 4:  # 4 kriter varsa
            # Cronbach's Alpha hesapla
            items = pivot.values
            n_items = items.shape[1]
            
            # Her item'Ä±n varyansÄ±
            item_variances = np.var(items, axis=0, ddof=1)
            total_variance = np.var(items.sum(axis=1), ddof=1)
            
            alpha = (n_items / (n_items - 1)) * (1 - item_variances.sum() / total_variance)
            
            results.append({
                'model': model,
                'cronbach_alpha': alpha,
                'n_items': n_items,
                'n_observations': len(pivot)
            })
    
    alpha_df = pd.DataFrame(results)
    return alpha_df

def sample_size_adequacy(df):
    """Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ yeterliliÄŸi"""
    
    n_participants = df['participant_id'].nunique()
    n_items = len(df.groupby(['word', 'model', 'sentence', 'criterion']))
    n_models = df['model'].nunique()
    n_words = df['word'].nunique()
    total_ratings = len(df)
    
    # Her model iÃ§in minimum deÄŸerlendirme
    ratings_per_model = df.groupby('model').size()
    
    return {
        'n_participants': n_participants,
        'n_items': n_items,
        'n_models': n_models,
        'n_words': n_words,
        'total_ratings': total_ratings,
        'ratings_per_model': ratings_per_model,
        'min_ratings_per_model': ratings_per_model.min(),
        'max_ratings_per_model': ratings_per_model.max()
    }

def normality_tests(df):
    """Normallik testleri"""
    
    results = []
    
    for model in df['model'].unique():
        model_data = df[df['model'] == model]['rating']
        
        # Shapiro-Wilk testi
        if len(model_data) >= 3:
            statistic, p_value = shapiro(model_data)
            results.append({
                'model': model,
                'test': 'Shapiro-Wilk',
                'statistic': statistic,
                'p_value': p_value,
                'is_normal': p_value > 0.05
            })
    
    return pd.DataFrame(results)

def variance_homogeneity_test(df):
    """Varyans homojenliÄŸi testi (Levene's Test)"""
    
    # Modeller arasÄ± varyans homojenliÄŸi
    groups = [df[df['model'] == model]['rating'].values for model in df['model'].unique()]
    
    statistic, p_value = levene(*groups)
    
    return {
        'test': "Levene's Test",
        'statistic': statistic,
        'p_value': p_value,
        'homogeneous': p_value > 0.05
    }

def statistical_significance_tests(df):
    """Ä°statistiksel anlamlÄ±lÄ±k testleri"""
    
    # Modeller arasÄ± fark (Kruskal-Wallis - non-parametric)
    groups = [df[df['model'] == model]['rating'].values for model in df['model'].unique()]
    
    h_statistic, p_value = kruskal(*groups)
    
    return {
        'test': 'Kruskal-Wallis H',
        'statistic': h_statistic,
        'p_value': p_value,
        'significant': p_value < 0.05
    }

def effect_size_analysis(df):
    """Etki bÃ¼yÃ¼klÃ¼ÄŸÃ¼ analizi (Cohen's d benzeri)"""
    
    # Model ortalamalarÄ±
    model_means = df.groupby('model')['rating'].mean().sort_values(ascending=False)
    
    best_model = model_means.index[0]
    worst_model = model_means.index[-1]
    
    best_data = df[df['model'] == best_model]['rating']
    worst_data = df[df['model'] == worst_model]['rating']
    
    # Cohen's d
    mean_diff = best_data.mean() - worst_data.mean()
    pooled_std = np.sqrt((best_data.var() + worst_data.var()) / 2)
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
    
    return {
        'best_model': best_model,
        'worst_model': worst_model,
        'mean_difference': mean_diff,
        'cohens_d': cohens_d,
        'effect_size_interpretation': interpret_cohens_d(cohens_d)
    }

def interpret_cohens_d(d):
    """Cohen's d yorumlama"""
    d = abs(d)
    if d < 0.2:
        return "Ã‡ok kÃ¼Ã§Ã¼k (negligible)"
    elif d < 0.5:
        return "KÃ¼Ã§Ã¼k (small)"
    elif d < 0.8:
        return "Orta (medium)"
    else:
        return "BÃ¼yÃ¼k (large)"

def response_bias_analysis(df):
    """YanÄ±t yanlÄ±lÄ±ÄŸÄ± analizi"""
    
    # Her katÄ±lÄ±mcÄ±nÄ±n genel eÄŸilimi
    participant_means = df.groupby('participant_id')['rating'].agg(['mean', 'std', 'count'])
    
    # Extreme scorers (Ã§ok yÃ¼ksek veya Ã§ok dÃ¼ÅŸÃ¼k puan verenler)
    extreme_high = participant_means[participant_means['mean'] > 4.5]
    extreme_low = participant_means[participant_means['mean'] < 2.5]
    
    # Standart sapmasÄ± Ã§ok dÃ¼ÅŸÃ¼k olanlar (hep aynÄ± puanÄ± verenler)
    low_variance = participant_means[participant_means['std'] < 0.5]
    
    return {
        'total_participants': len(participant_means),
        'extreme_high_scorers': len(extreme_high),
        'extreme_low_scorers': len(extreme_low),
        'low_variance_scorers': len(low_variance),
        'participant_mean_stats': participant_means['mean'].describe()
    }

def generate_report(df):
    """KapsamlÄ± rapor oluÅŸtur"""
    
    report = []
    report.append("# Bilimsel GeÃ§erlilik ve GÃ¼venilirlik Analizi Raporu\n")
    report.append("**Analiz Tarihi:** " + pd.Timestamp.now().strftime('%d.%m.%Y %H:%M') + "\n\n")
    
    # 1. Ã–RNEKLEM YETERLÄ°LÄ°ÄÄ°
    report.append("## 1. Ã–rneklem BÃ¼yÃ¼klÃ¼ÄŸÃ¼ ve Yeterlilik\n\n")
    sample_info = sample_size_adequacy(df)
    
    report.append(f"### Temel Ä°statistikler\n")
    report.append(f"- **Toplam KatÄ±lÄ±mcÄ±:** {sample_info['n_participants']} kiÅŸi\n")
    report.append(f"- **Toplam DeÄŸerlendirme:** {sample_info['total_ratings']} adet\n")
    report.append(f"- **Model SayÄ±sÄ±:** {sample_info['n_models']} adet\n")
    report.append(f"- **Kelime SayÄ±sÄ±:** {sample_info['n_words']} adet\n")
    report.append(f"- **Her Model iÃ§in DeÄŸerlendirme:** {sample_info['min_ratings_per_model']} - {sample_info['max_ratings_per_model']} arasÄ±\n\n")
    
    # Ã–rneklem yeterliliÄŸi yorumu
    report.append("### Ã–rneklem YeterliliÄŸi DeÄŸerlendirmesi\n\n")
    if sample_info['n_participants'] >= 15:
        report.append("âœ… **YETERLÄ°:** KatÄ±lÄ±mcÄ± sayÄ±sÄ± (n=16) insan deÄŸerlendirmesi Ã§alÄ±ÅŸmalarÄ± iÃ§in kabul edilebilir dÃ¼zeydedir.\n")
    else:
        report.append("âš ï¸ **SINIRDA:** KatÄ±lÄ±mcÄ± sayÄ±sÄ± artÄ±rÄ±labilir.\n")
    
    report.append("\n**LiteratÃ¼r KarÅŸÄ±laÅŸtÄ±rmasÄ±:**\n")
    report.append("- Benzer Ã§alÄ±ÅŸmalarda 10-30 katÄ±lÄ±mcÄ± yaygÄ±ndÄ±r\n")
    report.append("- Her item iÃ§in 15-20 deÄŸerlendirme ideal kabul edilir\n")
    report.append("- Bu Ã§alÄ±ÅŸmada her cÃ¼mle 16 kiÅŸi tarafÄ±ndan deÄŸerlendirilmiÅŸtir âœ…\n\n")
    
    # 2. GÃœVENÄ°LÄ°RLÄ°K ANALÄ°ZÄ°
    report.append("## 2. GÃ¼venilirlik Analizi\n\n")
    
    # Cronbach's Alpha
    report.append("### 2.1. Ä°Ã§ TutarlÄ±lÄ±k (Cronbach's Alpha)\n\n")
    alpha_results = calculate_cronbach_alpha(df)
    
    report.append("| Model | Cronbach's Î± | Yorumlama | GÃ¶zlem SayÄ±sÄ± |\n")
    report.append("|-------|--------------|-----------|---------------|\n")
    
    for _, row in alpha_results.iterrows():
        interpretation = interpret_alpha(row['cronbach_alpha'])
        report.append(f"| {row['model']} | {row['cronbach_alpha']:.3f} | {interpretation} | {row['n_observations']} |\n")
    
    report.append("\n**Cronbach's Alpha Yorumlama:**\n")
    report.append("- Î± â‰¥ 0.9: MÃ¼kemmel\n")
    report.append("- 0.8 â‰¤ Î± < 0.9: Ä°yi\n")
    report.append("- 0.7 â‰¤ Î± < 0.8: Kabul Edilebilir\n")
    report.append("- 0.6 â‰¤ Î± < 0.7: ÅÃ¼pheli\n")
    report.append("- Î± < 0.6: Kabul Edilemez\n\n")
    
    avg_alpha = alpha_results['cronbach_alpha'].mean()
    if avg_alpha >= 0.7:
        report.append(f"âœ… **SONUÃ‡:** Ortalama Î± = {avg_alpha:.3f} - DeÄŸerlendirme kriterleri arasÄ± tutarlÄ±lÄ±k KABUL EDÄ°LEBÄ°LÄ°R dÃ¼zeydedir.\n\n")
    else:
        report.append(f"âš ï¸ **SONUÃ‡:** Ortalama Î± = {avg_alpha:.3f} - Ä°Ã§ tutarlÄ±lÄ±k dÃ¼ÅŸÃ¼k, kriterler gÃ¶zden geÃ§irilmelidir.\n\n")
    
    # DeÄŸerlendiriciler arasÄ± gÃ¼venilirlik
    report.append("### 2.2. DeÄŸerlendiriciler ArasÄ± GÃ¼venilirlik\n\n")
    reliability = calculate_inter_rater_reliability(df)
    
    # Ortalama Coefficient of Variation
    avg_cv = reliability['cv'].mean()
    report.append(f"- **Ortalama Variation Coefficient:** {avg_cv:.2f}%\n")
    report.append(f"- **Ortalama Standart Sapma:** {reliability['std'].mean():.3f}\n\n")
    
    if avg_cv < 30:
        report.append(f"âœ… **YETERLÄ°:** CV = {avg_cv:.1f}% - DeÄŸerlendiriciler arasÄ± tutarlÄ±lÄ±k iyidir.\n\n")
    elif avg_cv < 50:
        report.append(f"âš ï¸ **ORTA:** CV = {avg_cv:.1f}% - DeÄŸerlendiriciler arasÄ± orta dÃ¼zey tutarlÄ±lÄ±k.\n\n")
    else:
        report.append(f"âŒ **DÃœÅÃœK:** CV = {avg_cv:.1f}% - DeÄŸerlendiriciler arasÄ± tutarlÄ±lÄ±k dÃ¼ÅŸÃ¼k.\n\n")
    
    # 3. NORMALLÄ°K VE VARYANS HOMOJENLÄ°ÄÄ°
    report.append("## 3. Ä°statistiksel VarsayÄ±mlar\n\n")
    
    # Normallik
    report.append("### 3.1. Normallik Testleri (Shapiro-Wilk)\n\n")
    normality = normality_tests(df)
    
    normal_count = normality['is_normal'].sum()
    total_count = len(normality)
    
    report.append(f"**SonuÃ§:** {normal_count}/{total_count} model normal daÄŸÄ±lÄ±m gÃ¶steriyor.\n\n")
    
    if normal_count < total_count / 2:
        report.append("âš ï¸ **NOT:** Veriler normal daÄŸÄ±lmÄ±yor, NON-PARAMETRIC testler kullanÄ±lmalÄ±dÄ±r.\n\n")
    else:
        report.append("âœ… Ã‡oÄŸunluk normal daÄŸÄ±lÄ±m gÃ¶steriyor.\n\n")
    
    # Varyans homojenliÄŸi
    report.append("### 3.2. Varyans HomojenliÄŸi (Levene's Test)\n\n")
    levene_result = variance_homogeneity_test(df)
    
    report.append(f"- **Test Ä°statistiÄŸi:** {levene_result['statistic']:.4f}\n")
    report.append(f"- **p-deÄŸeri:** {levene_result['p_value']:.4f}\n")
    
    if levene_result['homogeneous']:
        report.append(f"- **SonuÃ§:** âœ… Varyanslar homojen (p > 0.05)\n\n")
    else:
        report.append(f"- **SonuÃ§:** âš ï¸ Varyanslar homojen deÄŸil (p < 0.05)\n\n")
    
    # 4. Ä°STATÄ°STÄ°KSEL ANLAMLILIK
    report.append("## 4. Ä°statistiksel AnlamlÄ±lÄ±k Testleri\n\n")
    
    sig_test = statistical_significance_tests(df)
    
    report.append(f"### Kruskal-Wallis H Testi (Modeller ArasÄ± Fark)\n\n")
    report.append(f"- **H Ä°statistiÄŸi:** {sig_test['statistic']:.4f}\n")
    report.append(f"- **p-deÄŸeri:** {sig_test['p_value']:.6f}\n")
    
    if sig_test['significant']:
        report.append(f"- **SonuÃ§:** âœ… Modeller arasÄ±nda **Ä°STATÄ°STÄ°KSEL OLARAK ANLAMLI** fark vardÄ±r (p < 0.05)\n\n")
    else:
        report.append(f"- **SonuÃ§:** âŒ Modeller arasÄ±nda istatistiksel olarak anlamlÄ± fark yoktur (p â‰¥ 0.05)\n\n")
    
    # 5. ETKÄ° BÃœYÃœKLÃœÄÃœ
    report.append("## 5. Etki BÃ¼yÃ¼klÃ¼ÄŸÃ¼ Analizi\n\n")
    
    effect = effect_size_analysis(df)
    
    report.append(f"### En Ä°yi vs En KÃ¶tÃ¼ Model KarÅŸÄ±laÅŸtÄ±rmasÄ±\n\n")
    report.append(f"- **En Ä°yi Model:** {effect['best_model']}\n")
    report.append(f"- **En KÃ¶tÃ¼ Model:** {effect['worst_model']}\n")
    report.append(f"- **Ortalama Puan FarkÄ±:** {effect['mean_difference']:.3f}\n")
    report.append(f"- **Cohen's d:** {effect['cohens_d']:.3f}\n")
    report.append(f"- **Etki BÃ¼yÃ¼klÃ¼ÄŸÃ¼:** {effect['effect_size_interpretation']}\n\n")
    
    # 6. YANILIK ANALÄ°ZÄ°
    report.append("## 6. YanÄ±t YanlÄ±lÄ±ÄŸÄ± (Response Bias) Analizi\n\n")
    
    bias = response_bias_analysis(df)
    
    report.append(f"### KatÄ±lÄ±mcÄ± Puanlama EÄŸilimleri\n\n")
    report.append(f"- **Toplam KatÄ±lÄ±mcÄ±:** {bias['total_participants']}\n")
    report.append(f"- **AÅŸÄ±rÄ± YÃ¼ksek Puan Verenler (>4.5):** {bias['extreme_high_scorers']} kiÅŸi\n")
    report.append(f"- **AÅŸÄ±rÄ± DÃ¼ÅŸÃ¼k Puan Verenler (<2.5):** {bias['extreme_low_scorers']} kiÅŸi\n")
    report.append(f"- **DÃ¼ÅŸÃ¼k Varyans GÃ¶sterenler (std<0.5):** {bias['low_variance_scorers']} kiÅŸi\n\n")
    
    report.append("**KatÄ±lÄ±mcÄ± Ortalama PuanlarÄ±:**\n")
    report.append(f"- Min: {bias['participant_mean_stats']['min']:.2f}\n")
    report.append(f"- Max: {bias['participant_mean_stats']['max']:.2f}\n")
    report.append(f"- Ortalama: {bias['participant_mean_stats']['mean']:.2f}\n")
    report.append(f"- Std. Sapma: {bias['participant_mean_stats']['std']:.2f}\n\n")
    
    # 7. GENEL DEÄERLENDÄ°RME
    report.append("## 7. Genel DeÄŸerlendirme ve Ã–neriler\n\n")
    
    report.append("### âœ… GÃ¼Ã§lÃ¼ YÃ¶nler\n\n")
    
    strengths = []
    weaknesses = []
    
    if sample_info['n_participants'] >= 15:
        strengths.append("Yeterli katÄ±lÄ±mcÄ± sayÄ±sÄ± (n=16)")
    
    if avg_alpha >= 0.7:
        strengths.append(f"Ä°yi iÃ§ tutarlÄ±lÄ±k (Î±={avg_alpha:.3f})")
    else:
        weaknesses.append(f"DÃ¼ÅŸÃ¼k iÃ§ tutarlÄ±lÄ±k (Î±={avg_alpha:.3f})")
    
    if avg_cv < 40:
        strengths.append(f"Kabul edilebilir deÄŸerlendirici tutarlÄ±lÄ±ÄŸÄ± (CV={avg_cv:.1f}%)")
    else:
        weaknesses.append(f"DeÄŸerlendirici tutarlÄ±lÄ±ÄŸÄ± dÃ¼ÅŸÃ¼k (CV={avg_cv:.1f}%)")
    
    if sig_test['significant']:
        strengths.append("Modeller arasÄ± istatistiksel olarak anlamlÄ± fark (p<0.05)")
    else:
        weaknesses.append("Modeller arasÄ± fark istatistiksel olarak anlamlÄ± deÄŸil")
    
    if bias['extreme_high_scorers'] + bias['extreme_low_scorers'] < sample_info['n_participants'] * 0.3:
        strengths.append("DÃ¼ÅŸÃ¼k yanÄ±t yanlÄ±lÄ±ÄŸÄ±")
    else:
        weaknesses.append("BazÄ± katÄ±lÄ±mcÄ±larda aÅŸÄ±rÄ± puanlama eÄŸilimi var")
    
    for s in strengths:
        report.append(f"- {s}\n")
    
    report.append("\n### âš ï¸ Dikkat Edilmesi Gerekenler\n\n")
    
    for w in weaknesses:
        report.append(f"- {w}\n")
    
    # Ã–neriler
    report.append("\n### ğŸ“‹ Metodolojik Ã–neriler\n\n")
    
    if sample_info['n_participants'] < 30:
        report.append("1. **Ã–rneklem BÃ¼yÃ¼klÃ¼ÄŸÃ¼:** Ä°deal olarak 25-30 katÄ±lÄ±mcÄ±ya ulaÅŸÄ±lmasÄ± Ã¶nerilir\n")
    
    if avg_cv > 35:
        report.append("2. **DeÄŸerlendirici EÄŸitimi:** Puanlama Ã¶ncesi deÄŸerlendiricilere yÃ¶nerge ve Ã¶rnek eÄŸitimi verilmeli\n")
    
    if not normality['is_normal'].all():
        report.append("3. **Ä°statistiksel Testler:** Parametrik olmayan testler (Kruskal-Wallis, Mann-Whitney U) kullanÄ±lmalÄ±\n")
    
    if bias['low_variance_scorers'] > 0:
        report.append("4. **Veri Kalitesi:** TÃ¼m sorulara aynÄ± cevabÄ± veren katÄ±lÄ±mcÄ±lar incelenmeli\n")
    
    report.append("\n### ğŸ¯ SonuÃ§\n\n")
    
    # Genel deÄŸerlendirme skoru
    score = 0
    max_score = 5
    
    if sample_info['n_participants'] >= 15: score += 1
    if avg_alpha >= 0.7: score += 1
    if avg_cv < 40: score += 1
    if sig_test['significant']: score += 1
    if bias['extreme_high_scorers'] + bias['extreme_low_scorers'] < sample_info['n_participants'] * 0.3: score += 1
    
    percentage = (score / max_score) * 100
    
    report.append(f"**Bilimsel GeÃ§erlilik Skoru: {score}/{max_score} ({percentage:.0f}%)**\n\n")
    
    if percentage >= 80:
        report.append("âœ… **SONUÃ‡:** Bu Ã§alÄ±ÅŸmanÄ±n sonuÃ§larÄ± **BÄ°LÄ°MSEL OLARAK GEÃ‡ERLÄ° ve GÃœVENÄ°LÄ°R** kabul edilebilir.\n")
        report.append("Veriler akademik yayÄ±nlarda kullanÄ±labilir dÃ¼zeydedir.\n\n")
    elif percentage >= 60:
        report.append("âš ï¸ **SONUÃ‡:** Ã‡alÄ±ÅŸma **KABUL EDÄ°LEBÄ°LÄ°R** dÃ¼zeydedir ancak bazÄ± iyileÅŸtirmeler Ã¶nerilir.\n")
        report.append("SÄ±nÄ±rlÄ±lÄ±klar belirtilerek akademik yayÄ±nlarda kullanÄ±labilir.\n\n")
    else:
        report.append("âŒ **SONUÃ‡:** Ã‡alÄ±ÅŸmanÄ±n metodolojik aÃ§Ä±dan **GÃœÃ‡LENDÄ°RÄ°LMESÄ°** gerekir.\n")
        report.append("Ã–ncelikle yukarÄ±daki Ã¶nerilerin uygulanmasÄ± tavsiye edilir.\n\n")
    
    report.append("---\n\n")
    report.append("*Bu rapor otomatik olarak oluÅŸturulmuÅŸtur ve uzman gÃ¶rÃ¼ÅŸÃ¼ ile desteklenmelidir.*\n")
    
    return ''.join(report)

def interpret_alpha(alpha):
    """Cronbach's Alpha yorumlama"""
    if alpha >= 0.9:
        return "MÃ¼kemmel âœ…"
    elif alpha >= 0.8:
        return "Ä°yi âœ…"
    elif alpha >= 0.7:
        return "Kabul Edilebilir âš ï¸"
    elif alpha >= 0.6:
        return "ÅÃ¼pheli âš ï¸"
    else:
        return "Kabul Edilemez âŒ"

def main():
    """Ana fonksiyon"""
    print("ğŸ”¬ Bilimsel GeÃ§erlilik Analizi BaÅŸlatÄ±lÄ±yor...\n")
    
    # Veriyi yÃ¼kle
    df = load_data()
    print(f"âœ… {len(df)} kayÄ±t yÃ¼klendi\n")
    
    # Rapor oluÅŸtur
    print("ğŸ“Š Analizler yapÄ±lÄ±yor...")
    report = generate_report(df)
    
    # Kaydet
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… Rapor oluÅŸturuldu: {OUTPUT_FILE}\n")
    
    # KÄ±sa Ã¶zet yazdÄ±r
    print("=" * 80)
    print("HIZLI Ã–ZET")
    print("=" * 80)
    
    sample_info = sample_size_adequacy(df)
    alpha_results = calculate_cronbach_alpha(df)
    sig_test = statistical_significance_tests(df)
    
    print(f"ğŸ“Š KatÄ±lÄ±mcÄ± SayÄ±sÄ±: {sample_info['n_participants']}")
    print(f"ğŸ“Š Toplam DeÄŸerlendirme: {sample_info['total_ratings']}")
    print(f"ğŸ“Š Ortalama Cronbach's Î±: {alpha_results['cronbach_alpha'].mean():.3f}")
    print(f"ğŸ“Š Modeller arasÄ± fark anlamlÄ± mÄ±? {'EVET âœ…' if sig_test['significant'] else 'HAYIR âŒ'}")
    print(f"ğŸ“Š p-deÄŸeri: {sig_test['p_value']:.6f}")
    print("=" * 80)
    
    print("\nâœ¨ Analiz tamamlandÄ±!")

if __name__ == "__main__":
    main()
