{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f72f6a6-fdbd-48a8-8e82-f9df315d8187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================  START A2  ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing A2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:24<00:00, 14.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ A2: 60 satƒ±r kaydedildi ‚Üí /home/user/Documents/Tez/Deneyler/LLM_Degerlendirme/data/ratings/deepseek_ratings/ratings_A2.json\n",
      "\n",
      "================================  START B1  ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing B1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:19<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B1: 60 satƒ±r kaydedildi ‚Üí /home/user/Documents/Tez/Deneyler/LLM_Degerlendirme/data/ratings/deepseek_ratings/ratings_B1.json\n",
      "\n",
      "================================  START B2  ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing B2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:29<00:00, 14.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B2: 60 satƒ±r kaydedildi ‚Üí /home/user/Documents/Tez/Deneyler/LLM_Degerlendirme/data/ratings/deepseek_ratings/ratings_B2.json\n",
      "\n",
      "================================  START C1  ================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:20<00:00, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ C1: 60 satƒ±r kaydedildi ‚Üí /home/user/Documents/Tez/Deneyler/LLM_Degerlendirme/data/ratings/deepseek_ratings/ratings_C1.json\n",
      "\n",
      "üéâ Bitti: A2‚ÜíC1 deƒüerlendirmeleri yazƒ±ldƒ±.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# --- Ortam deƒüi≈ükenlerini y√ºkle ---\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# --- Sabitler / Ayarlar ---\n",
    "LEVELS = [\"A2\", \"B1\", \"B2\", \"C1\"]  # A1'i zaten √ºrettin\n",
    "BASE_URL = \"https://api.deepseek.com\"\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "TEMPERATURE = 1.0                  # 0.0-1.0 arasƒ± deƒüer, 1.0 daha yaratƒ±cƒ± sonu√ßlar verir\n",
    "N_EVALS = 2                        # her task i√ßin ka√ß tekrar\n",
    "\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"DEEPSEEK_API_KEY ortam deƒüi≈ükeni tanƒ±mlƒ± deƒüil!\")\n",
    "\n",
    "# --- ƒ∞stemci ---\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# --- Yollar ---\n",
    "root = os.getcwd()\n",
    "tasks_dir = os.path.abspath(os.path.join(root, \"..\", \"data\", \"tasks\"))\n",
    "ratings_root = os.path.abspath(os.path.join(root, \"..\", \"data\", \"ratings\"))\n",
    "output_dir = os.path.join(ratings_root, \"deepseek_ratings\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Ayrƒ±≈ütƒ±rma ---\n",
    "def parse_response(response_text: str) -> Dict[str, Dict[str, int]]:\n",
    "    pattern = r\"Sentence\\s*([A-F])\\s*:\\s*([1-5])\\s*,\\s*([1-5])\\s*,\\s*([1-5])\\s*,\\s*([1-5])\"\n",
    "    results = {}\n",
    "    for m in re.finditer(pattern, response_text):\n",
    "        label = f\"Sentence {m.group(1)}\"\n",
    "        s = list(map(int, m.groups()[1:]))\n",
    "        results[label] = {\"word_usage\": s[0], \"clarity\": s[1], \"grammar\": s[2], \"naturalness\": s[3]}\n",
    "    return results\n",
    "\n",
    "# --- G√ºvenli √ßaƒürƒ± ---\n",
    "def call_deepseek(prompt: str, retries: int = 3, backoff: float = 2.0) -> str:\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=TEMPERATURE,\n",
    "                stream=False,\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"[HATA] API √ßaƒürƒ±sƒ± ba≈üarƒ±sƒ±z (deneme {attempt}/{retries}): {e}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(backoff ** attempt)\n",
    "            else:\n",
    "                raise\n",
    "    return \"\"\n",
    "\n",
    "def average_scores(score_dicts: List[Dict[str, Dict[str, int]]]) -> Dict[str, Dict[str, float]]:\n",
    "    merged: Dict[str, Dict[str, List[float]]] = {}\n",
    "    for sd in score_dicts:\n",
    "        for label, metrics in sd.items():\n",
    "            merged.setdefault(label, {k: [] for k in [\"word_usage\",\"clarity\",\"grammar\",\"naturalness\"]})\n",
    "            for k, v in metrics.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    merged[label][k].append(float(v))\n",
    "    averaged: Dict[str, Dict[str, float]] = {}\n",
    "    for label, lists in merged.items():\n",
    "        averaged[label] = {k: (sum(vals)/len(vals) if vals else 0.0) for k, vals in lists.items()}\n",
    "    return averaged\n",
    "\n",
    "def process_level(level: str):\n",
    "    tasks_path = os.path.join(tasks_dir, f\"tasks_{level}.json\")\n",
    "    out_path   = os.path.join(output_dir, f\"ratings_{level}.json\")\n",
    "\n",
    "    if not os.path.exists(tasks_path):\n",
    "        print(f\"[ATLA] Task dosyasƒ± yok: {tasks_path}\")\n",
    "        return\n",
    "\n",
    "    with open(tasks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tasks = json.load(f)\n",
    "\n",
    "    all_ratings = []\n",
    "    for task in tqdm(tasks, desc=f\"Processing {level}\"):\n",
    "        prompt  = task[\"prompt\"]\n",
    "        mapping = task[\"mapping\"]\n",
    "        task_id = task[\"task_id\"]\n",
    "        word    = task[\"word\"]\n",
    "\n",
    "        try:\n",
    "            runs = []\n",
    "            for _ in range(N_EVALS):\n",
    "                reply = call_deepseek(prompt)\n",
    "                runs.append(parse_response(reply))\n",
    "\n",
    "            averaged = average_scores(runs)\n",
    "\n",
    "            for label, rating in averaged.items():\n",
    "                if label not in mapping:\n",
    "                    print(f\"[UYARI] E≈üle≈ümeyen etiket: {label} (task_id={task_id})\"); continue\n",
    "                all_ratings.append({\n",
    "                    \"task_id\":  task_id,\n",
    "                    \"model\":    mapping[label][\"model\"],\n",
    "                    \"level\":    level,\n",
    "                    \"word\":     word,\n",
    "                    \"label\":    label,\n",
    "                    \"sentence\": mapping[label][\"sentence\"],\n",
    "                    \"ratings\":  {\n",
    "                        \"word_usage\": round(rating.get(\"word_usage\", 0.0), 3),\n",
    "                        \"clarity\":     round(rating.get(\"clarity\", 0.0), 3),\n",
    "                        \"grammar\":     round(rating.get(\"grammar\", 0.0), 3),\n",
    "                        \"naturalness\": round(rating.get(\"naturalness\", 0.0), 3),\n",
    "                    }\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"[HATA] Level={level} Task={task_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_ratings, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ {level}: {len(all_ratings)} satƒ±r kaydedildi ‚Üí {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for lvl in LEVELS:\n",
    "        print(\"\\n\" + \"=\"*32 + f\"  START {lvl}  \" + \"=\"*32)\n",
    "        process_level(lvl)\n",
    "    print(\"\\nüéâ Bitti: A2‚ÜíC1 deƒüerlendirmeleri yazƒ±ldƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae4905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "[1/2] TASK ID: A1_age | LEVEL=A1 | WORD=age\n",
      "------------------------------------------------------------------------------------------\n",
      ">> PROMPT (ilk 600 karakter):\n",
      "You are a professional CEFR-aligned English sentence evaluator.\n",
      "\n",
      "Your task is to evaluate 6 example sentences that all use the target word: \"age\" at CEFR level: A1.\n",
      "\n",
      "Rate each sentence from 1 (poor) to 5 (excellent) for the following **four independent criteria**:\n",
      "\n",
      "1. **Word Usage** ‚Äì Is the target word used correctly and meaningfully in context?\n",
      "2. **Clarity** ‚Äì Is the sentence understandable and suitable for the given CEFR level?\n",
      "3. **Grammar** ‚Äì Is the grammar accurate and appropriate for the level?\n",
      "4. **Naturalness** ‚Äì Does the sentence sound fluent and natural to a native speaker?\n",
      "\n",
      "‚ö†Ô∏è Important Instructions:\n",
      "- **Only return numerical ratings** for each criterion.\n",
      "- **Do not include any explanations, comments, or justifications.**\n",
      "- Follow the exact output format below.\n",
      "\n",
      "### Output Format:\n",
      "Sentence A: <Word Usage>, <Clarity>, <Grammar>, <Naturalness>  \n",
      "Sentence B: ...  \n",
      "Sentence C: ...  \n",
      "Sentence D: ...  \n",
      "Sentence E: ...  \n",
      "Sentence F: ...  \n",
      "\n",
      "### Sentences:\n",
      "Sentence A: My birthday is on January 12th.\n",
      "Sentence B: My age is twenty-five years old.\n",
      "Sentence C: I am 20 years old.\n",
      "Sentence D: I am 10 years old, and I love playing with my pet dog.\n",
      "Sentence E: My age is seven years old.\n",
      "Sentence F: I am ten years old.\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      ">> MAPPING LABELS: ['Sentence A', 'Sentence B', 'Sentence C', 'Sentence D', 'Sentence E', 'Sentence F']\n",
      ">> MAPPING SENTENCE PREVIEW:\n",
      "   Sentence A: My birthday is on January 12th.\n",
      "   Sentence B: My age is twenty-five years old.\n",
      "   Sentence C: I am 20 years old.\n",
      "   Sentence D: I am 10 years old, and I love playing with my pet dog.\n",
      "   Sentence E: My age is seven years old.\n",
      "   Sentence F: I am ten years old.\n",
      "------------------------------------------------------------------------------------------\n",
      ">> RAW REPLY:\n",
      "Sentence A: 1, 3, 4, 4  \n",
      "Sentence B: 3, 3, 3, 3  \n",
      "Sentence C: 5, 5, 5, 5  \n",
      "Sentence D: 4, 5, 5, 5  \n",
      "Sentence E: 3, 3, 3, 3  \n",
      "Sentence F: 5, 5, 5, 5\n",
      "------------------------------------------------------------------------------------------\n",
      ">> PARSED LABELS: ['Sentence A', 'Sentence B', 'Sentence C', 'Sentence D', 'Sentence E', 'Sentence F']\n",
      "‚úÖ PARSE OK & LABELS MATCH & SCORES IN RANGE\n",
      "‚Üí Sentence A | model=Llama-3.2-8B-Instruct/Llama-3.2-8B-Instruct-Q4_K_M.gguf | My birthday is on January 12th.\n",
      "   scores: {'word_usage': 1, 'clarity': 3, 'grammar': 4, 'naturalness': 4}\n",
      "‚Üí Sentence B | model=Claude Sonnet 4 | My age is twenty-five years old.\n",
      "   scores: {'word_usage': 3, 'clarity': 3, 'grammar': 3, 'naturalness': 3}\n",
      "==========================================================================================\n",
      "[2/2] TASK ID: A1_animal | LEVEL=A1 | WORD=animal\n",
      "------------------------------------------------------------------------------------------\n",
      ">> PROMPT (ilk 600 karakter):\n",
      "You are a professional CEFR-aligned English sentence evaluator.\n",
      "\n",
      "Your task is to evaluate 6 example sentences that all use the target word: \"animal\" at CEFR level: A1.\n",
      "\n",
      "Rate each sentence from 1 (poor) to 5 (excellent) for the following **four independent criteria**:\n",
      "\n",
      "1. **Word Usage** ‚Äì Is the target word used correctly and meaningfully in context?\n",
      "2. **Clarity** ‚Äì Is the sentence understandable and suitable for the given CEFR level?\n",
      "3. **Grammar** ‚Äì Is the grammar accurate and appropriate for the level?\n",
      "4. **Naturalness** ‚Äì Does the sentence sound fluent and natural to a native speaker?\n",
      "\n",
      "‚ö†Ô∏è Important Instructions:\n",
      "- **Only return numerical ratings** for each criterion.\n",
      "- **Do not include any explanations, comments, or justifications.**\n",
      "- Follow the exact output format below.\n",
      "\n",
      "### Output Format:\n",
      "Sentence A: <Word Usage>, <Clarity>, <Grammar>, <Naturalness>  \n",
      "Sentence B: ...  \n",
      "Sentence C: ...  \n",
      "Sentence D: ...  \n",
      "Sentence E: ...  \n",
      "Sentence F: ...  \n",
      "\n",
      "### Sentences:\n",
      "Sentence A: This is a dog, a cat, and a bird. They are animals.\n",
      "Sentence B: A cat is a small animal.\n",
      "Sentence C: The dog is a friendly animal.\n",
      "Sentence D: I have a pet animal called Max.\n",
      "Sentence E: My favorite animal is the elephant, which is very big and grey.\n",
      "Sentence F: The cat is a pet animal.\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      ">> MAPPING LABELS: ['Sentence A', 'Sentence B', 'Sentence C', 'Sentence D', 'Sentence E', 'Sentence F']\n",
      ">> MAPPING SENTENCE PREVIEW:\n",
      "   Sentence A: This is a dog, a cat, and a bird. They are animals.\n",
      "   Sentence B: A cat is a small animal.\n",
      "   Sentence C: The dog is a friendly animal.\n",
      "   Sentence D: I have a pet animal called Max.\n",
      "   Sentence E: My favorite animal is the elephant, which is very big and grey.\n",
      "   Sentence F: The cat is a pet animal.\n",
      "------------------------------------------------------------------------------------------\n",
      ">> RAW REPLY:\n",
      "Sentence A: 5, 5, 5, 5  \n",
      "Sentence B: 5, 5, 5, 5  \n",
      "Sentence C: 5, 5, 5, 5  \n",
      "Sentence D: 4, 5, 5, 4  \n",
      "Sentence E: 5, 4, 4, 4  \n",
      "Sentence F: 5, 5, 5, 5\n",
      "------------------------------------------------------------------------------------------\n",
      ">> PARSED LABELS: ['Sentence A', 'Sentence B', 'Sentence C', 'Sentence D', 'Sentence E', 'Sentence F']\n",
      "‚úÖ PARSE OK & LABELS MATCH & SCORES IN RANGE\n",
      "‚Üí Sentence A | model=Ministral-8B-Instruct-2410.Q4_K_M.gguf | This is a dog, a cat, and a bird. They are animals.\n",
      "   scores: {'word_usage': 5, 'clarity': 5, 'grammar': 5, 'naturalness': 5}\n",
      "‚Üí Sentence B | model=Gemini 2.5 Flash | A cat is a small animal.\n",
      "   scores: {'word_usage': 5, 'clarity': 5, 'grammar': 5, 'naturalness': 5}\n",
      "==========================================================================================\n",
      "üéâ Test tamam: 2 g√∂rev i√ßin prompt ‚Üí reply ‚Üí parse ‚Üí mapping doƒürulandƒ±.\n"
     ]
    }
   ],
   "source": [
    "# === DEBUG: 2 g√∂revle u√ßtan uca test h√ºcresi ===\n",
    "\n",
    "import os, json, re, textwrap\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---- Ayarlar ----\n",
    "LEVEL = \"A1\"  # burada test etmek istediƒüin seviyeyi se√ß (A1, A2, B1, B2, C1)\n",
    "BASE_URL = \"https://api.deepseek.com\"\n",
    "MODEL_NAME = \"deepseek-chat\"\n",
    "\n",
    "# ---- Ortam / ƒ∞stemci ----\n",
    "load_dotenv(find_dotenv())\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"DEEPSEEK_API_KEY bulunamadƒ±. .env veya ortam deƒüi≈ükenini kontrol et.\")\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# ---- Yollar ----\n",
    "root = os.getcwd()\n",
    "tasks_path = os.path.abspath(os.path.join(root, \"..\", \"data\", \"tasks\", f\"tasks_{LEVEL}.json\"))\n",
    "\n",
    "# ---- Parser ----\n",
    "def parse_response(response_text: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Beklenen satƒ±rlar:\n",
    "      Sentence A: 4, 4, 5, 4\n",
    "    D√∂nen dict anahtarlarƒ±: \"Sentence A\" ... \"Sentence F\"\n",
    "    \"\"\"\n",
    "    pattern = r\"Sentence\\s*([A-F])\\s*:\\s*([1-5])\\s*,\\s*([1-5])\\s*,\\s*([1-5])\\s*,\\s*([1-5])\"\n",
    "    results = {}\n",
    "    for m in re.finditer(pattern, response_text):\n",
    "        label = f\"Sentence {m.group(1)}\"\n",
    "        scores = list(map(int, m.groups()[1:]))\n",
    "        results[label] = {\n",
    "            \"word_usage\":  scores[0],\n",
    "            \"clarity\":     scores[1],\n",
    "            \"grammar\":     scores[2],\n",
    "            \"naturalness\": scores[3],\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def assert_parsed_ok(parsed, mapping, task_id):\n",
    "    # Etiket e≈üle≈ümesi\n",
    "    expected = set(mapping.keys())                 # {\"Sentence A\", ..., \"Sentence F\"}\n",
    "    got = set(parsed.keys())\n",
    "    missing = expected - got\n",
    "    extra   = got - expected\n",
    "    if missing or extra:\n",
    "        raise ValueError(f\"[PARSE MISMATCH] task={task_id} missing={sorted(missing)} extra={sorted(extra)}\")\n",
    "    # Skor aralƒ±ƒüƒ±\n",
    "    for lbl, r in parsed.items():\n",
    "        for k, v in r.items():\n",
    "            assert isinstance(v, int) and 1 <= v <= 5, f\"[SCORE RANGE] task={task_id} {lbl}/{k} -> {v}\"\n",
    "\n",
    "# ---- Yardƒ±mcƒ±: kƒ±sa c√ºmle √∂nizleme ----\n",
    "def preview(s: str, n=80):\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    return s if len(s) <= n else s[:n] + \"...\"\n",
    "\n",
    "# ---- √áalƒ±≈ütƒ±r ----\n",
    "with open(tasks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    tasks = json.load(f)\n",
    "\n",
    "tasks = tasks[:2]  # SADECE ƒ∞LK 2 G√ñREV\n",
    "\n",
    "for idx, task in enumerate(tasks, start=1):\n",
    "    prompt  = task[\"prompt\"]\n",
    "    mapping = task[\"mapping\"]\n",
    "    task_id = task[\"task_id\"]\n",
    "    word    = task[\"word\"]\n",
    "    level   = task[\"level\"]\n",
    "\n",
    "    print(\"=\"*90)\n",
    "    print(f\"[{idx}/2] TASK ID: {task_id} | LEVEL={level} | WORD={word}\")\n",
    "    print(\"-\"*90)\n",
    "    print(\">> PROMPT (ilk 600 karakter):\")\n",
    "    #print(textwrap.shorten(prompt, width=600, placeholder=\" ...\"))\n",
    "    print(prompt)\n",
    "    print(\"-\"*90)\n",
    "    print(\">> MAPPING LABELS:\", list(mapping.keys()))\n",
    "    print(\">> MAPPING SENTENCE PREVIEW:\")\n",
    "    for lbl in [\"Sentence A\",\"Sentence B\",\"Sentence C\",\"Sentence D\",\"Sentence E\",\"Sentence F\"]:\n",
    "        sent = mapping[lbl][\"sentence\"]\n",
    "        print(f\"   {lbl}: {preview(sent)}\")\n",
    "\n",
    "    # --- API √ßaƒürƒ±sƒ± ---\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        stream=False,\n",
    "    )\n",
    "    reply = resp.choices[0].message.content\n",
    "\n",
    "    print(\"-\"*90)\n",
    "    print(\">> RAW REPLY:\")\n",
    "    print(reply)\n",
    "\n",
    "    # --- Parse & doƒürulama ---\n",
    "    parsed = parse_response(reply)\n",
    "    print(\"-\"*90)\n",
    "    print(\">> PARSED LABELS:\", list(parsed.keys()))\n",
    "    assert_parsed_ok(parsed, mapping, task_id)\n",
    "    print(\"‚úÖ PARSE OK & LABELS MATCH & SCORES IN RANGE\")\n",
    "\n",
    "    # --- E≈üle≈ütirilmi≈ü √∂rnek √ßƒ±ktƒ± (ilk 2 label g√∂sterelim) ---\n",
    "    show_labels = [\"Sentence A\",\"Sentence B\"]\n",
    "    for lbl in show_labels:\n",
    "        model_name = mapping[lbl][\"model\"]\n",
    "        sentence   = mapping[lbl][\"sentence\"]\n",
    "        scores     = parsed[lbl]\n",
    "        print(f\"‚Üí {lbl} | model={model_name} | {preview(sentence)}\")\n",
    "        print(f\"   scores: {scores}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"üéâ Test tamam: 2 g√∂rev i√ßin prompt ‚Üí reply ‚Üí parse ‚Üí mapping doƒürulandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8d36c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2be3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cbe9887",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
