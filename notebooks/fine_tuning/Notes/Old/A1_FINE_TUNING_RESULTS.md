# A1 Seviyesi Fine-Tuning SonuÃ§larÄ±

**Tarih:** 24 Ekim 2025  
**Model:** Llama-3.2-1B-Instruct  
**CEFR Seviyesi:** A1 (Beginner)  
**DonanÄ±m:** NVIDIA RTX 4090 24GB

---

## ğŸ“Š 1. VERÄ° SETÄ° HAZIRLIÄI

### Dataset Ä°statistikleri:
- **Toplam Ã–rnek SayÄ±sÄ±:** 2000
- **Training Set:** 1800 Ã¶rnek (90%)
- **Evaluation Set:** 200 Ã¶rnek (10%)
- **Format:** Alpaca prompt format (JSON)
- **Split Seed:** 42 (reproducibility iÃ§in)

### Ã–rnek Veri FormatÄ±:
```json
{
  "instruction": "Write a sentence using the word 'happy'",
  "output": "I am happy today."
}
```

### Alpaca Format Åablonu:
```
### Instruction:
{instruction}

### Response:
{output}
```

### Veri Karakteristikleri:
- **Ortalama Uzunluk:** 116 kelime
- **En Uzun Ã–rnek:** 134 kelime
- **En KÄ±sa Ã–rnek:** 77 kelime
- **Token UzunluÄŸu:** TÃ¼m Ã¶rnekler 512 token'Ä±n altÄ±nda

### Veri DosyalarÄ±:
```
formatted_data/A1/training_data_a1_train.json  (1800 Ã¶rnek)
formatted_data/A1/training_data_a1_eval.json   (200 Ã¶rnek)
```

---

## ğŸ”§ 2. FINE-TUNING YAPISI

### Temel Model:
- **Model:** meta-llama/Llama-3.2-1B-Instruct
- **Base Model Path:** `/media/.../text-generation-webui/user_data/models/meta-llama_Llama-3.2-1B-Instruct`
- **Model Boyutu:** 1 Billion parameters

### Fine-tuning Framework:
- **KÃ¼tÃ¼phane:** Unsloth (v2025.10.9)
- **Trainer:** SFTTrainer (Supervised Fine-Tuning)
- **Neden Unsloth?**
  - text-generation-webui Training PRO'da eval dataset desteÄŸi yok
  - Unsloth daha hÄ±zlÄ± ve hafÄ±za verimli
  - Eval dataset desteÄŸi var
  - 4-bit quantization desteÄŸi

### LoRA (Low-Rank Adaptation) Parametreleri:
```python
r (rank) = 128
lora_alpha = 256
lora_dropout = 0.05
bias = "none"
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                  "gate_proj", "up_proj", "down_proj"]
```

**LoRA AÃ§Ä±klamasÄ±:**
- Full model yerine sadece kÃ¼Ã§Ã¼k adaptÃ¶r katmanlarÄ± eÄŸitiliyor
- Orijinal model aÄŸÄ±rlÄ±klarÄ± sabit kalÄ±yor
- %99 daha az parametre eÄŸitiliyor
- VRAM kullanÄ±mÄ± Ã§ok dÃ¼ÅŸÃ¼k

### Training Hyperparameters:
```python
max_seq_length = 2048
load_in_4bit = True
dtype = bf16 (RTX 4090 iÃ§in)

per_device_train_batch_size = 16
per_device_eval_batch_size = 16
gradient_accumulation_steps = 4
effective_batch_size = 64  (16 Ã— 4)

num_train_epochs = 3
learning_rate = 2e-4
warmup_steps = 100
weight_decay = 0.01
max_grad_norm = 1.0

optimizer = "adamw_torch"
lr_scheduler_type = "cosine"
```

### Evaluation Strategy:
```python
eval_strategy = "steps"
eval_steps = 50
save_steps = 50
save_total_limit = 5
load_best_model_at_end = True
metric_for_best_model = "eval_loss"
```

---

## ğŸ“ˆ 3. EÄÄ°TÄ°M SONUÃ‡LARI

### Training Metrics (Response-only Loss):
```
Initial Train Loss:  1.95
Final Train Loss:    0.64
Loss Reduction:      67%

Final Eval Loss:     0.82
Training Time:       93 seconds (~1.5 dakika)
```

**Not:** SFTTrainer otomatik olarak "labels masking" yapÄ±yor. Yani loss hesaplanÄ±rken sadece **response kÄ±smÄ±** dikkate alÄ±nÄ±yor, instruction kÄ±smÄ± ignore ediliyor.

### Training Progress:
```
Step    Train Loss    Eval Loss
----    ----------    ---------
0       1.9500        -
10      1.7200        -
50      1.2300        0.9500
87      0.6400        0.8200 (best)
```

### En Ä°yi Model:
- **Checkpoint:** checkpoint-87 (son adÄ±m)
- **Eval Loss:** 0.82
- **SeÃ§im Kriteri:** En dÃ¼ÅŸÃ¼k eval_loss
- **Model Tipi:** LoRA adaptÃ¶rleri

### Output Modelleri:
1. **LoRA Model:**
   - Path: `/media/.../loras/llama1b-a1-unsloth`
   - Format: Sadece LoRA adaptÃ¶rleri
   - Boyut: ~200MB

2. **Merged Model:**
   - Path: `/media/.../models/llama1b-a1-unsloth_merged`
   - Format: Base model + LoRA adaptÃ¶rleri birleÅŸtirilmiÅŸ
   - Boyut: ~2.5GB
   - text-generation-webui'de direkt kullanÄ±labilir

---

## ğŸ¯ 4. BASELINE KARÅILAÅTIRMASI

### DeÄŸerlendirme YÃ¶ntemi:
- **Dataset:** AynÄ± eval dataset (200 Ã¶rnek)
- **Loss Hesaplama:** **Full text** (instruction + response)
- **Metrikler:** Loss ve Perplexity
- **Neden full text?** Baseline ile adil karÅŸÄ±laÅŸtÄ±rma iÃ§in

**Ã–nemli Fark:**
- Training sÄ±rasÄ±ndaki eval loss (0.82): **Sadece response**
- Baseline comparison loss (1.47): **TÃ¼m text**

### SonuÃ§lar:

| Model          | Eval Loss | Perplexity | Ä°yileÅŸme |
|----------------|-----------|------------|----------|
| Baseline       | 2.63      | 13.86      | -        |
| Fine-tuned A1  | 1.47      | 4.35       | **44.1%**|

### Metrik AÃ§Ä±klamalarÄ±:

**Loss (KayÄ±p):**
- Model'in ne kadar yanÄ±ldÄ±ÄŸÄ±nÄ± Ã¶lÃ§er
- DÃ¼ÅŸÃ¼k loss = iyi model
- Cross-entropy loss kullanÄ±lÄ±r: $Loss = -\log(P_{doÄŸru\_token})$

**Perplexity (ÅaÅŸkÄ±nlÄ±k):**
- Loss'un daha anlaÅŸÄ±lÄ±r hali
- FormÃ¼l: $Perplexity = e^{Loss}$
- Baseline: 13.86 â†’ Model 14 kelime arasÄ±nda kararsÄ±z
- Fine-tuned: 4.35 â†’ Model 4 kelime arasÄ±nda kararsÄ±z
- DÃ¼ÅŸÃ¼k perplexity = daha gÃ¼venli tahminler

### Ä°yileÅŸme OranlarÄ±:
```
Loss Reduction:       44.1%
Perplexity Reduction: 68.6%
```

**Yorum:** Fine-tuned model, A1 formatÄ±nÄ± ve iÃ§eriÄŸini baseline modele gÃ¶re Ã§ok daha iyi anlÄ±yor ve tahminlerde daha gÃ¼venli.

---

## ğŸ’¡ 5. TEKNÄ°K DETAYLAR

### Loss Hesaplama MantÄ±ÄŸÄ±:

Model **kelime kelime tahmin** yapÄ±yor:

```python
# Ã–rnek text:
"### Instruction:\nWrite a sentence\n### Response:\nI am happy"

# Model her token iÃ§in:
for token in text:
    context = previous_tokens
    prediction_probability = model.predict(token | context)
    loss += -log(prediction_probability)
```

**Training sÄ±rasÄ±nda (Response-only):**
```python
# Labels masking yapÄ±lÄ±yor
labels = [-100, -100, ...,     # Instruction (ignored)
          "I", "am", "happy"]  # Response (used for loss)
# Final loss: 0.82
```

**Evaluation sÄ±rasÄ±nda (Full text):**
```python
# Masking yok
labels = ["###", "Instruction", ..., "I", "am", "happy"]
# Final loss: 1.47
```

### Quantization:
- **4-bit Quantization:** Model aÄŸÄ±rlÄ±klarÄ± 32-bit yerine 4-bit'te tutuluyor
- **VRAM Tasarrufu:** ~4x daha az bellek
- **Performans:** Minimal kayÄ±p (~1-2%)

### Gradient Checkpointing:
- Training sÄ±rasÄ±nda tÃ¼m activations saklanmÄ±yor
- Backward pass'te gerektiÄŸinde yeniden hesaplanÄ±yor
- VRAM tasarrufu saÄŸlÄ±yor

---

## ğŸ“ 6. TEZ Ä°Ã‡Ä°N KULLANIM Ã–NERÄ°LERÄ°

### YÃ¶ntem (Methodology) BÃ¶lÃ¼mÃ¼:

```
3.3 Fine-tuning SÃ¼reci

Llama-3.2-1B-Instruct modeli A1 seviyesi iÃ§in 2000 Ã¶rneklik 
dataset ile fine-tune edilmiÅŸtir. Dataset %90 train, %10 eval 
olarak bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r.

Low-Rank Adaptation (LoRA) tekniÄŸi kullanÄ±larak parametrelerin 
sadece kÃ¼Ã§Ã¼k bir kÄ±smÄ± (rank=128) eÄŸitilmiÅŸ, bu sayede 
hesaplama maliyeti minimize edilmiÅŸtir.

EÄŸitim NVIDIA RTX 4090 24GB GPU Ã¼zerinde Unsloth kÃ¼tÃ¼phanesi 
ile gerÃ§ekleÅŸtirilmiÅŸtir. 4-bit quantization ve gradient 
checkpointing teknikleri ile VRAM kullanÄ±mÄ± optimize edilmiÅŸtir.

Hyperparameters:
- Learning rate: 2e-4 (cosine scheduler)
- Batch size: 64 (effective)
- Epochs: 3
- Optimizer: AdamW
```

### SonuÃ§lar (Results) BÃ¶lÃ¼mÃ¼:

```
4.1 A1 Seviyesi EÄŸitim SonuÃ§larÄ±

Model 1800 Ã¶rneklik A1 dataset'i ile baÅŸarÄ±yla eÄŸitilmiÅŸtir.
Training loss 1.95'ten 0.64'e dÃ¼ÅŸerek %67 iyileÅŸme gÃ¶stermiÅŸtir.
Evaluation loss 0.82 seviyesinde sona ermiÅŸtir.
EÄŸitim sÃ¼resi 93 saniye olarak gerÃ§ekleÅŸmiÅŸtir.

4.2 Baseline KarÅŸÄ±laÅŸtÄ±rmasÄ±

Fine-tuned model'in etkinliÄŸini deÄŸerlendirmek iÃ§in baseline 
(eÄŸitilmemiÅŸ) model ile karÅŸÄ±laÅŸtÄ±rma yapÄ±lmÄ±ÅŸtÄ±r:

[Tablo 4.1: Baseline vs Fine-tuned KarÅŸÄ±laÅŸtÄ±rmasÄ±]
Model          | Eval Loss | Perplexity | Ä°yileÅŸme
---------------|-----------|------------|----------
Baseline       | 2.63      | 13.86      | -
Fine-tuned A1  | 1.47      | 4.35       | 44.1%

Fine-tuned model baseline'a gÃ¶re %44.1 daha dÃ¼ÅŸÃ¼k loss deÄŸeri 
gÃ¶stermiÅŸtir. Perplexity deÄŸerindeki %68.6'lÄ±k dÃ¼ÅŸÃ¼ÅŸ, modelin 
tahminlerinde Ã¶nemli Ã¶lÃ§Ã¼de gÃ¼ven kazandÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.

Bu sonuÃ§lar, fine-tuning iÅŸleminin A1 seviyesi iÃ§erik Ã¼retiminde 
modeli baÅŸarÄ±yla adapte ettiÄŸini kanÄ±tlamaktadÄ±r.
```

### TartÄ±ÅŸma (Discussion) BÃ¶lÃ¼mÃ¼:

```
5.1 Fine-tuning EtkinliÄŸi

A1 seviyesinde elde edilen %44.1'lik iyileÅŸme, LoRA tabanlÄ± 
fine-tuning yaklaÅŸÄ±mÄ±nÄ±n etkili olduÄŸunu gÃ¶stermektedir.

Training eval loss (0.82) ile full text eval loss (1.47) 
arasÄ±ndaki fark beklenen bir durumdur Ã§Ã¼nkÃ¼:
- Training sÄ±rasÄ±nda sadece response kÄ±smÄ±ndan loss hesaplanÄ±r
- Baseline comparison'da ise tÃ¼m text deÄŸerlendirilir
- Bu iki metrik farklÄ± yetenekleri Ã¶lÃ§mektedir

Perplexity deÄŸerindeki dramatik dÃ¼ÅŸÃ¼ÅŸ (13.86 â†’ 4.35), modelin 
A1 seviyesi iÃ§erik Ã¼retiminde Ã§ok daha gÃ¼venli hale geldiÄŸini 
gÃ¶stermektedir.
```

---

## ğŸ”„ 7. DÄ°ÄER SEVÄ°YELER Ä°Ã‡Ä°N TEKRAR EDÄ°LECEK ADIMLAR

A2, B1, B2, C1 seviyeleri iÃ§in aynÄ± prosedÃ¼r:

1. **Dataset HazÄ±rlÄ±ÄŸÄ±:**
   ```bash
   python formatted_data/split_train_eval.py training_data_{level}.json
   ```

2. **train_with_unsloth.py GÃ¼ncellemeleri:**
   - Dataset path'lerini deÄŸiÅŸtir (A1 â†’ A2/B1/B2/C1)
   - LORA_NAME'i gÃ¼ncelle (Ã¶rn: "llama1b-a2-unsloth")
   
3. **Training:**
   ```bash
   python train_with_unsloth.py
   ```

4. **Baseline Comparison:**
   - baseline_comparison.py'de EVAL_DATA ve FINETUNED_MODEL path'lerini gÃ¼ncelle
   ```bash
   python baseline_comparison.py
   ```

5. **SonuÃ§larÄ± Kaydet:**
   - Her seviye iÃ§in ayrÄ± results dosyasÄ±
   - TÃ¼m metrikleri karÅŸÄ±laÅŸtÄ±rmalÄ± tablo

---

## ğŸ“ 8. DOSYA YAPISI

```
notebooks/fine_tuning/
â”œâ”€â”€ formatted_data/
â”‚   â””â”€â”€ A1/
â”‚       â”œâ”€â”€ training_data_a1_train.json    (1800 Ã¶rnekler)
â”‚       â””â”€â”€ training_data_a1_eval.json     (200 Ã¶rnekler)
â”‚
â”œâ”€â”€ train_with_unsloth.py                  (Ana eÄŸitim script'i)
â”œâ”€â”€ baseline_comparison.py                 (Baseline karÅŸÄ±laÅŸtÄ±rma)
â”œâ”€â”€ baseline_comparison_results.txt        (SonuÃ§lar)
â”œâ”€â”€ A1_FINE_TUNING_RESULTS.md             (Bu dÃ¶kÃ¼man)
â”‚
â””â”€â”€ [Output Models]
    â”œâ”€â”€ /media/.../loras/llama1b-a1-unsloth/
    â”‚   â”œâ”€â”€ adapter_config.json
    â”‚   â”œâ”€â”€ adapter_model.safetensors
    â”‚   â””â”€â”€ ...
    â”‚
    â””â”€â”€ /media/.../models/llama1b-a1-unsloth_merged/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â””â”€â”€ ...
```

---

## ğŸ¯ 9. Ã–NEMLÄ° NOTLAR

### Training vs Evaluation Loss FarkÄ±:
- **Training Eval Loss (0.82):** Sadece response tokens, model'in response Ã¼retme kabiliyeti
- **Baseline Comparison Loss (1.47):** TÃ¼m text tokens, model'in genel anlama kabiliyeti
- **Ä°kisi de doÄŸru ve Ã¶nemli**, farklÄ± ÅŸeyleri Ã¶lÃ§Ã¼yorlar!

### Max Sequence Length:
- Training: 2048 token
- Evaluation: 2048 token (ama A1 Ã¶rnekleri zaten kÄ±sa, max 200 token)
- SonuÃ§: max_length deÄŸiÅŸimi A1 iÃ§in sonuÃ§larÄ± etkilemiyor

### Model SeÃ§imi:
- Inference iÃ§in **merged model** kullanÄ±lmalÄ±
- LoRA model sadece base model ile birlikte Ã§alÄ±ÅŸÄ±r
- Merged model text-generation-webui'de direkt yÃ¼klenebilir

### Reproducibility:
- Seed: 3407 (training)
- Seed: 42 (data split)
- TÃ¼m hyperparameters sabit
- AynÄ± sonuÃ§lar tekrar Ã¼retilebilir

---

## âœ… 10. BAÅARI KRÄ°TERLERÄ°

âœ… **EÄŸitim BaÅŸarÄ±lÄ±:**
- Loss 1.95'ten 0.64'e dÃ¼ÅŸtÃ¼ (67% iyileÅŸme)
- Eval loss stabil (0.82)
- Overfitting yok (train/eval loss dengeli)

âœ… **Baseline'dan ÃœstÃ¼n:**
- %44.1 daha dÃ¼ÅŸÃ¼k loss
- %68.6 daha dÃ¼ÅŸÃ¼k perplexity
- AÃ§Ä±k ve Ã¶lÃ§Ã¼lebilir iyileÅŸme

âœ… **Teknik Olarak SaÄŸlam:**
- LoRA ile verimli eÄŸitim
- Eval dataset ile proper validation
- Reproducible sonuÃ§lar

âœ… **Tez iÃ§in Yeterli:**
- Nicel metrikler mevcut
- Ä°nsan uzman deÄŸerlendirmesi eklenecek
- LLM deÄŸerlendirmesi eklenecek
- ÃœÃ§lÃ¼ validation gÃ¼Ã§lÃ¼ tez oluÅŸturur

---

**SonuÃ§:** A1 seviyesi fine-tuning baÅŸarÄ±yla tamamlandÄ± ve tez iÃ§in kullanÄ±labilir seviyede sonuÃ§lar elde edildi. ğŸ‰
