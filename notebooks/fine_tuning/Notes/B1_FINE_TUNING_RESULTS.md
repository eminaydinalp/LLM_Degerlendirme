# B1 Level Fine-Tuning Results

**Tarih:** 26 Ekim 2025  
**Model:** Llama-3.2-1B-Instruct â†’ llama1b-b1-unsloth-v1  
**Method:** LoRA with Unsloth

---

## ğŸ“Š Dataset

- **Format:** List format (10 words â†’ 10 sentences per example)
- **Train Set:** 1,800 examples (90%)
- **Eval Set:** 200 examples (10%)
- **Total Words:** 806 unique B1-level words

---

## âš™ï¸ Hyperparameters

```python
max_seq_length = 512  # B1 iÃ§in yeterli (max ~290 tokens)
batch_size = 16
gradient_accumulation_steps = 4  # Effective batch size: 64
learning_rate = 2e-4
num_train_epochs = 10
warmup_ratio = 0.1
lora_rank = 128
lora_alpha = 256
lora_dropout = 0.05
```

---

## ğŸ¯ Training Results

- **Duration:** 6 dakika 42 saniye (402.4 sec)
- **Total Steps:** 290
- **Train samples/sec:** 44.73
- **Hardware:** NVIDIA RTX 4090 24GB

### Loss Progression

| Epoch | Train Loss | Eval Loss | Note |
|-------|-----------|-----------|------|
| 0.35  | 1.82      | -         | BaÅŸlangÄ±Ã§ |
| 1.74  | -         | 0.61      | Ä°lk eval |
| 3.46  | -         | 0.45      | HÄ±zlÄ± iyileÅŸme |
| 5.18  | -         | 0.43      | En iyi bÃ¶lge |
| 6.92  | -         | 0.43      | **En iyi nokta** |
| 8.64  | -         | 0.44      | Hafif overfitting |
| 10.0  | 0.34      | -         | Son |

**Best Model:** Epoch 6.92 (eval_loss = 0.426)

---

## ğŸ“ˆ Baseline Comparison

Test set: `training_data_b1_list_format_eval.json` (200 examples)

| Model | Eval Loss | Perplexity | Ä°yileÅŸme |
|-------|-----------|------------|----------|
| **Baseline (Untrained)** | 2.34 | 10.34 | - |
| **Fine-tuned B1** | 0.73 | 2.08 | **68.7%** â†“ |

### Ã–nemli Metrikler

- âœ… **Loss Reduction:** 68.7% (En yÃ¼ksek!)
- âœ… **Perplexity Reduction:** 79.9% (En yÃ¼ksek!)
- âœ… **Training Loss:** 1.82 â†’ 0.34 (81.3% improvement)

---

## ğŸ’¾ Saved Models

**LoRA Adapters:**
```
/media/.../loras/llama1b-b1-unsloth-v1/
```

**Merged Model (16-bit):**
```
/media/.../models/llama1b-b1-unsloth-v1_merged/
```

**TensorBoard Logs:**
```
/media/.../loras/llama1b-b1-unsloth-v1/runs/
```

---

## ğŸ” Analysis

### Strengths
- âœ… **En yÃ¼ksek iyileÅŸme oranlarÄ±** (Loss: 68.7%, Perplexity: 79.9%)
- âœ… Ã‡ok stabil eÄŸitim (gradient norm dÃ¼ÅŸÃ¼k)
- âœ… Epoch 6.92'de mÃ¼kemmel convergence
- âœ… Minimal overfitting

### Observations
- B1 cÃ¼mleleri daha uzun ve kompleks olmasÄ±na raÄŸmen 512 token yeterli
- Perplexity reduction neredeyse %80 - model Ã§ok gÃ¼venli tahminler yapÄ±yor
- Training loss A1/A2'den daha fazla dÃ¼ÅŸtÃ¼ (0.34)

---

## ğŸ“Š A1 vs A2 vs B1 Comprehensive Comparison

| Metrik | A1 | A2 | B1 |
|--------|----|----|-----|
| **Train Examples** | 160 | 1,800 | 1,800 |
| **Unique Words** | ~178 | 867 | 806 |
| **Max Token Length** | ~230 | ~231 | ~290 |
| **Loss Reduction** | ~65% | 67.4% | **68.7%** â­ |
| **Perplexity Reduction** | ~77% | 79.1% | **79.9%** â­ |
| **Training Time** | ~5 min | ~6 min | ~7 min |
| **Final Train Loss** | 0.41 | 0.38 | **0.34** â­ |
| **Best Eval Loss** | 0.57 | 0.51 | **0.43** â­ |

### ğŸ¯ Key Insights:
1. **Daha fazla data = Daha iyi sonuÃ§lar** (160 â†’ 1,800 Ã¶rnekler)
2. **B1 tÃ¼m metriklerde en iyi performansÄ± gÃ¶sterdi**
3. **Seviyeler arasÄ± tutarlÄ±lÄ±k** - fine-tuning her seviye iÃ§in etkili
4. **Token length farkÄ± minimal etkili** - 512 max_seq_length her seviye iÃ§in yeterli

---

## âœ… Next Steps

1. âœ… B1 model test edildi
2. âœ… Baseline comparison yapÄ±ldÄ±
3. âœ… Training plots oluÅŸturuldu
4. ğŸ”œ B2 seviyesi hazÄ±rlanabilir
5. ğŸ”œ Kalite deÄŸerlendirmesi (human evaluation)

---

## ğŸ“ Conclusion

B1 seviyesi fine-tuning **son derece baÅŸarÄ±lÄ±**! Model, baseline'a kÄ±yasla **68.7% daha dÃ¼ÅŸÃ¼k loss** ve **79.9% daha dÃ¼ÅŸÃ¼k perplexity** ile:
- TÃ¼m seviyeler arasÄ±nda **en iyi iyileÅŸme oranlarÄ±nÄ±** elde etti
- B1 seviyesi Ä°ngilizce cÃ¼mle Ã¼retimi iÃ§in optimize edildi
- Kompleks cÃ¼mlelere raÄŸmen stabil ve gÃ¼venilir performans gÃ¶steriyor

**Model hazÄ±r:** `llama1b-b1-unsloth-v1_merged` ğŸš€

---

## ğŸ“Š Training Plots Location

```
training_plots/B1/
â”œâ”€â”€ training_loss.png
â”œâ”€â”€ eval_loss.png
â”œâ”€â”€ combined_loss.png
â”œâ”€â”€ learning_rate.png
â””â”€â”€ gradient_norm.png
```

All plots saved at 300 DPI, ready for publication! ğŸ“ˆ
