# A2 Level Fine-Tuning Results

**Tarih:** 26 Ekim 2025  
**Model:** Llama-3.2-1B-Instruct â†’ llama1b-a2-unsloth-v1  
**Method:** LoRA with Unsloth

---

## ğŸ“Š Dataset

- **Format:** List format (10 words â†’ 10 sentences per example)
- **Train Set:** 1,800 examples (90%)
- **Eval Set:** 200 examples (10%)
- **Total Words:** 867 unique A2-level words

---

## âš™ï¸ Hyperparameters

```python
max_seq_length = 512
batch_size = 16
gradient_accumulation_steps = 4  # Effective batch size: 64
learning_rate = 2e-4
num_train_epochs = 10
warmup_ratio = 0.1
lora_rank = 128
lora_alpha = 256
lora_dropout = 0.05
```

---

## ğŸ¯ Training Results

- **Duration:** 5 dakika 46 saniye (346.6 sec)
- **Total Steps:** 290
- **Train samples/sec:** 51.93
- **Hardware:** NVIDIA RTX 4090 24GB

### Loss Progression

| Epoch | Train Loss | Eval Loss | Note |
|-------|-----------|-----------|------|
| 0.35  | 1.76      | -         | BaÅŸlangÄ±Ã§ |
| 1.74  | -         | 0.66      | Ä°lk eval |
| 3.46  | -         | 0.52      | Ä°yileÅŸme devam ediyor |
| 5.18  | -         | 0.51      | En iyi nokta |
| 6.92  | -         | 0.51      | Stabil |
| 8.64  | -         | 0.54      | Hafif overfitting |
| 10.0  | 0.38      | -         | Son |

**Best Model:** Epoch 6.92 (eval_loss = 0.508)

---

## ğŸ“ˆ Baseline Comparison

Test set: `training_data_a2_list_format_eval.json` (200 examples)

| Model | Eval Loss | Perplexity | Ä°yileÅŸme |
|-------|-----------|------------|----------|
| **Baseline (Untrained)** | 2.33 | 10.24 | - |
| **Fine-tuned A2** | 0.76 | 2.14 | **67.4%** â†“ |

### Ã–nemli Metrikler

- âœ… **Loss Reduction:** 67.4%
- âœ… **Perplexity Reduction:** 79.1%
- âœ… **Training Loss:** 1.76 â†’ 0.38 (78.4% improvement)

---

## ğŸ’¾ Saved Models

**LoRA Adapters:**
```
/media/.../loras/llama1b-a2-unsloth-v1/
```

**Merged Model (16-bit):**
```
/media/.../models/llama1b-a2-unsloth-v1_merged/
```

**TensorBoard Logs:**
```
/media/.../loras/llama1b-a2-unsloth-v1/runs/
```

---

## ğŸ” Analysis

### Strengths
- âœ… Ã‡ok gÃ¼Ã§lÃ¼ iyileÅŸme (67.4% loss reduction)
- âœ… Perplexity 10.24'ten 2.14'e dÃ¼ÅŸtÃ¼ (Ã§ok daha gÃ¼venli tahminler)
- âœ… HÄ±zlÄ± eÄŸitim (~6 dakika)
- âœ… 1,800 train Ã¶rneÄŸi ile zengin dataset

### Observations
- Epoch 6.92'de en iyi eval loss (0.508)
- Epoch 8-10 arasÄ±nda hafif overfitting belirtileri
- Early stopping epoch 6.92'de durabilirdi ama fark minimal

---

## ğŸ“Š A1 vs A2 Comparison

| Metrik | A1 | A2 | Fark |
|--------|----|----|------|
| **Train Examples** | 160 | 1,800 | **11.25x** daha fazla |
| **Unique Words** | ~178 | 867 | **4.87x** daha fazla |
| **Loss Reduction** | ~65% | 67.4% | +2.4% |
| **Perplexity Reduction** | ~77% | 79.1% | +2.1% |
| **Training Time** | ~5 min | ~6 min | Benzer |

**SonuÃ§:** Daha fazla veri ile daha iyi generalization! ğŸ¯

---

## âœ… Next Steps

1. âœ… A2 model test edildi
2. âœ… Baseline comparison yapÄ±ldÄ±
3. ğŸ”œ B1 seviyesi dataset hazÄ±rlanabilir
4. ğŸ”œ Kalite deÄŸerlendirmesi (human evaluation)

---

## ğŸ“ Conclusion

A2 seviyesi fine-tuning baÅŸarÄ±lÄ±! Model, baseline'a kÄ±yasla **67.4% daha dÃ¼ÅŸÃ¼k loss** ve **79.1% daha dÃ¼ÅŸÃ¼k perplexity** ile A2 seviyesi Ä°ngilizce cÃ¼mle Ã¼retimi iÃ§in optimize edildi.

**Model hazÄ±r:** `llama1b-a2-unsloth-v1_merged` ğŸš€
